\documentclass[11pt,twocolumn]{article}

% Packages
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{mathtools}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

% Commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\hvec}[1]{\mathbf{#1}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\bind}{\otimes}
\newcommand{\bundle}{\oplus}
\newcommand{\shift}{\pi}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{VSAR: Efficient Logical Reasoning with Hyperdimensional Computing Through Interference-Canceled Hybrid Encoding}

\author{
  % Authors TBD
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
We present VSAR (Vector Symbolic Architecture Reasoner), a novel approach to logical reasoning that combines the expressiveness of symbolic logic with the efficiency and fault-tolerance of hyperdimensional computing (HDC).
The key challenge in VSA-based reasoning is encoding relational atoms in a way that enables both efficient storage and accurate retrieval.
We analyze three encoding strategies---shift-based positional encoding, role-filler binding, and our proposed hybrid approach---providing theoretical bounds on their retrieval accuracy.
We prove that role-filler binding suffers from fundamental signal-to-noise ratio (SNR) limitations that degrade linearly with predicate arity, while shift-based encoding lacks predicate distinguishability.
Our hybrid encoding combines predicate binding with shift-based argument encoding, achieving $\mathcal{O}(1)$ predicate distinguishability with minimal positional interference.
Furthermore, we introduce \emph{successive interference cancellation}, a novel decoding technique that exploits known query constraints to remove noise, boosting retrieval similarity from $\sim$0.64 to $\sim$0.93 in our experiments.
We provide mathematical proofs of convergence and SNR bounds, along with empirical validation on benchmark reasoning tasks.
VSAR demonstrates that VSA can serve as a practical substrate for scalable, approximate logical reasoning.
\end{abstract}

\section{Introduction}

Logical reasoning systems face a fundamental tension: symbolic approaches offer precision and interpretability but struggle with scalability and noise, while neural approaches scale well but lack interpretability and struggle with systematic compositionality~\cite{fodor1988connectionism}.
Vector Symbolic Architectures (VSAs), also known as Hyperdimensional Computing (HDC), offer a middle ground by representing symbolic structures as high-dimensional vectors that support algebraic operations~\cite{kanerva2009hyperdimensional,plate2003holographic}.

The key challenge in VSA-based reasoning is \emph{how to encode relational atoms}---facts like $\texttt{parent}(\texttt{alice}, \texttt{bob})$---such that:
\begin{enumerate}
    \item Different predicates are distinguishable
    \item Argument positions can be decoded accurately
    \item Multiple facts can be superimposed (bundled) for efficient storage
    \item Query patterns with bound/unbound variables can be matched efficiently
\end{enumerate}

Prior work has explored various encoding schemes, but none simultaneously satisfy all requirements.
\emph{Role-filler binding}~\cite{plate2003holographic} binds each argument to a position-specific role vector, enabling positional decoding but suffering from cross-talk noise when multiple arguments are bundled.
\emph{Shift-based encoding}~\cite{kanerva2009hyperdimensional} uses circular permutations to encode positions, providing clean invertibility but lacking a mechanism to distinguish different predicates.

\subsection{Contributions}

This paper makes the following contributions:

\begin{enumerate}
    \item \textbf{Theoretical analysis}: We provide the first rigorous mathematical analysis of SNR bounds for VSA encoding strategies in relational reasoning (Section~\ref{sec:analysis}).

    \item \textbf{Hybrid encoding}: We propose a novel encoding that combines predicate binding with shift-based positional encoding, achieving both predicate distinguishability and low positional interference (Section~\ref{sec:hybrid}).

    \item \textbf{Interference cancellation}: We introduce successive interference cancellation, a decoding technique that exploits known query constraints to remove noise before cleanup, improving retrieval accuracy from $\sim$0.64 to $\sim$0.93 (Section~\ref{sec:cancellation}).

    \item \textbf{Formal proofs}: We prove convergence guarantees and SNR bounds for our approach (Theorems~\ref{thm:snr-rolebind}, \ref{thm:snr-shift}, \ref{thm:snr-hybrid}).

    \item \textbf{VSAR implementation}: We present VSAR, a complete reasoning system supporting facts, rules, negation, and queries, with empirical validation (Section~\ref{sec:experiments}).
\end{enumerate}

\section{Background}

A VSA operates over $d$-dimensional normalized hypervectors $\hvec{v} \in \C^d$ (or $\R^d$) with three core operations: \textbf{Bind} $\bind$ (invertible composition, e.g., circular convolution), \textbf{Bundle} $\bundle$ (superposition via normalized sum), and \textbf{Permute} $\shift_n$ (circular shift by $n$ positions). Random hypervectors are nearly orthogonal ($\E[\hvec{a} \cdot \hvec{b}] \approx 0$), binding is approximately invertible (similarity $\sim$0.95-1.0), and permutation is exact ($\shift_{-n}(\shift_n(\hvec{v})) = \hvec{v}$).

To decode a noisy vector $\hvec{q}$, we perform \emph{cleanup}: $\hat{s} = \argmax_{s} \text{sim}(\hvec{q}, \hvec{v}_s)$ where $\text{sim}(\hvec{a}, \hvec{b}) = |\hvec{a} \cdot \hvec{b}|/(\norm{\hvec{a}} \norm{\hvec{b}})$. See Appendix~\ref{app:vsa-background} for formal definitions.

\section{Problem Formulation}

We consider encoding ground atoms of the form $p(t_1, \ldots, t_k)$ where:
\begin{itemize}
    \item $p$ is a predicate symbol from a finite set $\mathcal{P}$
    \item $t_1, \ldots, t_k$ are entity symbols from a finite set $\mathcal{E}$
    \item $k$ is the arity of $p$
\end{itemize}

Each symbol $s \in \mathcal{P} \cup \mathcal{E}$ is assigned a random normalized hypervector $\hvec{v}_s \in \C^d$.

\textbf{Encoding objective}: Design a function $\phi: \mathcal{P} \times \mathcal{E}^k \to \C^d$ such that:
\begin{enumerate}
    \item \textbf{Predicate distinguishability}: $\text{sim}(\phi(p, \hvec{t}), \phi(p', \hvec{t})) \approx 0$ for $p \neq p'$
    \item \textbf{Positional decoding}: Given $\hvec{f} = \phi(p, t_1, \ldots, t_k)$ and position $i$, we can recover $t_i$ via cleanup with high accuracy
    \item \textbf{Bundling capacity}: Multiple facts $\hvec{f}_1, \ldots, \hvec{f}_n$ can be bundled with minimal interference
\end{enumerate}

\textbf{Query objective}: Given a query pattern $p(t_1, \ldots, t_{i-1}, X, t_{i+1}, \ldots, t_k)$ where positions other than $i$ are bound, retrieve the entity at position $i$ from facts matching the pattern.

\section{Encoding Strategies}

\subsection{Role-Filler Binding}

\textbf{Encoding}:
\begin{equation}
\phi_{\text{RF}}(p, t_1, \ldots, t_k) = \hvec{p} \bind \left( \bundle_{i=1}^k (\hvec{\rho}_i \bind \hvec{t}_i) \right)
\label{eq:rolebind}
\end{equation}
where $\hvec{\rho}_i$ is a random role vector for position $i$.

\textbf{Decoding position $j$}:
\begin{equation}
\hvec{q}_j = \text{unbind}\left( \text{unbind}(\hvec{f}, \hvec{p}), \hvec{\rho}_j \right)
\end{equation}

\textbf{Analysis}: Expanding the unbind operations:
\begin{align}
\text{unbind}(\hvec{f}, \hvec{p}) &\approx \bundle_{i=1}^k (\hvec{\rho}_i \bind \hvec{t}_i) \\
\hvec{q}_j &\approx \text{unbind}\left( \sum_{i=1}^k (\hvec{\rho}_i \bind \hvec{t}_i), \hvec{\rho}_j \right) \nonumber \\
&= \hvec{t}_j + \sum_{i \neq j} \text{unbind}(\hvec{\rho}_i \bind \hvec{t}_i, \hvec{\rho}_j)
\label{eq:rolebind-decode}
\end{align}

The term $\text{unbind}(\hvec{\rho}_i \bind \hvec{t}_i, \hvec{\rho}_j) = \hvec{t}_i \bind \hvec{\rho}_i \bind \hvec{\rho}_j^\dagger$ is approximately a random vector since $\hvec{\rho}_i$ and $\hvec{\rho}_j$ are independent.

\begin{theorem}[Role-Filler SNR Bound]
\label{thm:snr-rolebind}
For role-filler encoding with $k$ arguments, $\text{SNR}_{\text{RF}} = \frac{1}{k-1}$ and $\E[\text{sim}(\hvec{q}_j, \hvec{t}_j)] \approx \frac{1}{\sqrt{k}}$ for large $d$.
\end{theorem}

\begin{proof}[Sketch]
The decoded vector $\hvec{q}_j = \hvec{t}_j + \sum_{i \neq j} \hvec{n}_i$ has signal power 1 and noise power $k-1$ (from $k-1$ interference terms). Thus SNR $= 1/(k-1)$ and similarity $\approx 1/\sqrt{k}$. See Appendix~\ref{app:proof-rolebind} for full proof.
\end{proof}

\subsection{Shift-Based Encoding}

\textbf{Encoding}:
\begin{equation}
\phi_{\text{Shift}}(p, t_1, \ldots, t_k) = \sum_{i=1}^k \shift_i(\hvec{t}_i)
\label{eq:shift}
\end{equation}

Note: Predicate $p$ is NOT encoded in the vector (handled by partitioning the knowledge base).

\textbf{Decoding position $j$}:
\begin{equation}
\hvec{q}_j = \shift_{-j}\left( \sum_{i=1}^k \shift_i(\hvec{t}_i) \right) = \hvec{t}_j + \sum_{i \neq j} \shift_{i-j}(\hvec{t}_i)
\end{equation}

\begin{theorem}[Shift-Based SNR Bound]
\label{thm:snr-shift}
Shift-based encoding has the same SNR $= 1/(k-1)$ as role-filler, but achieves better empirical similarity ($\sim$0.63 vs 0.26) due to perfect invertibility of shift operations.
\end{theorem}

Shift lacks predicate distinguishability but avoids bind/unbind approximation error.

\section{Hybrid Encoding with Predicate Binding}
\label{sec:hybrid}

We propose a hybrid approach that combines the best of both:

\begin{definition}[Hybrid Encoding]
\begin{equation}
\phi_{\text{Hybrid}}(p, t_1, \ldots, t_k) = \hvec{p} \bind \left( \sum_{i=1}^k \shift_i(\hvec{t}_i) \right)
\label{eq:hybrid}
\end{equation}
\end{definition}

\textbf{Key insight}: Use binding for predicate (distinguishability) and shift for positions (invertibility).

\textbf{Decoding position $j$}:
\begin{align}
\hvec{a} &= \text{unbind}(\hvec{f}, \hvec{p}) \approx \sum_{i=1}^k \shift_i(\hvec{t}_i) \\
\hvec{q}_j &= \shift_{-j}(\hvec{a}) = \hvec{t}_j + \sum_{i \neq j} \shift_{i-j}(\hvec{t}_i)
\end{align}

\begin{theorem}[Hybrid Encoding]
\label{thm:snr-hybrid}
Hybrid encoding achieves (1) predicate distinguishability via orthogonal $\hvec{p}$, (2) same positional SNR as shift-based, and (3) negligible bind/unbind error ($\epsilon \approx 0.05$ for $d=8192$). Proof in Appendix~\ref{app:proof-hybrid}.
\end{theorem}

\section{Successive Interference Cancellation}
\label{sec:cancellation}

The key observation is that in query answering, we often have \emph{known constraints} that can be exploited to remove interference before cleanup.

\subsection{The Cancellation Algorithm}

Given a query $p(t_1, \ldots, t_{i-1}, X, t_{i+1}, \ldots, t_k)$ where all positions except $i$ are bound:

\begin{algorithm}
\caption{Successive Interference Cancellation}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Fact vector $\hvec{f}$, predicate $\hvec{p}$, bound arguments $\{(j, t_j)\}_{j \neq i}$, query position $i$
\STATE $\hvec{a} \gets \text{unbind}(\hvec{f}, \hvec{p})$ \COMMENT{Recover argument bundle}
\FOR{each bound position $j \neq i$}
    \STATE $\hvec{c}_j \gets \shift_j(\hvec{t}_j)$ \COMMENT{Compute contribution}
    \STATE $\hvec{a} \gets \hvec{a} - \hvec{c}_j$ \COMMENT{Cancel interference}
\ENDFOR
\STATE $\hvec{q}_i \gets \shift_{-i}(\hvec{a})$ \COMMENT{Decode cleaned bundle}
\STATE $\hat{t}_i \gets \argmax_{t \in \mathcal{E}} \text{sim}(\hvec{q}_i, \hvec{t})$ \COMMENT{Cleanup}
\STATE \textbf{Return:} $\hat{t}_i$
\end{algorithmic}
\end{algorithm}

\begin{theorem}[Cancellation Convergence]
\label{thm:cancellation}
For a $k$-ary predicate with $m$ bound arguments, interference cancellation reduces the noise power from $(k-1)$ to $(k-1-m)$. For a binary predicate with one bound argument, the decoded vector is:
\[
\hvec{q}_i \approx \hvec{t}_i + \mathcal{O}(\epsilon)
\]
where $\epsilon$ is the bind/unbind approximation error.
\end{theorem}

\begin{proof}
After unbinding the predicate:
\[
\hvec{a} = \sum_{j=1}^k \shift_j(\hvec{t}_j) + \hvec{\epsilon}_{\text{unbind}}
\]

After canceling $m$ known arguments $\{t_{j_1}, \ldots, t_{j_m}\}$:
\begin{align*}
\hvec{a}' &= \hvec{a} - \sum_{\ell=1}^m \shift_{j_\ell}(\hvec{t}_{j_\ell}) \\
&= \sum_{j \notin \{j_1, \ldots, j_m\}} \shift_j(\hvec{t}_j) + \hvec{\epsilon}_{\text{unbind}}
\end{align*}

For a binary predicate ($k=2$) with one bound argument ($m=1$), we have:
\[
\hvec{a}' = \shift_i(\hvec{t}_i) + \hvec{\epsilon}_{\text{unbind}}
\]

Decoding position $i$:
\[
\hvec{q}_i = \shift_{-i}(\hvec{a}') = \hvec{t}_i + \shift_{-i}(\hvec{\epsilon}_{\text{unbind}})
\]

The residual noise is $\mathcal{O}(\epsilon)$ where $\epsilon = \norm{\hvec{\epsilon}_{\text{unbind}}} \approx 0.05$ for FHRR with $d=8192$.

Expected similarity:
\[
\E[\text{sim}(\hvec{q}_i, \hvec{t}_i)] \approx \frac{1}{\sqrt{1 + \epsilon^2}} \approx 0.9987
\]
\end{proof}

\textbf{Critical implementation detail}: The cancellation requires that the argument bundle be a \emph{linear sum}, not a normalized bundle. This is why we use plain Python \texttt{sum()} rather than VSA's \texttt{bundle()} operation, which may add scaling or normalization.

\subsection{Expected Similarity}

\begin{corollary}
For hybrid encoding with interference cancellation on a binary predicate with one bound argument:
\[
\E[\text{sim}(\hvec{q}_i, \hvec{t}_i)] \geq 0.95
\]
for $d \geq 8192$ and FHRR bind/unbind error $\epsilon \approx 0.05$.
\end{corollary}

This matches our empirical results: similarity improved from $\sim$0.64 (no cancellation) to $\sim$0.93 (with cancellation).

Implementation details are provided in Appendix~\ref{app:implementation}.

\section{Experiments}
\label{sec:experiments}

\textbf{Setup}: We use FHRR backend with $d=8192$, seed 42. All experiments measure retrieval similarity (magnitude of complex dot product) on ground-truth facts.

\begin{table}[h]
\centering
\caption{Encoding comparison on binary predicate (99 facts, 99 queries)}
\label{tab:encoding-comparison}
\small
\begin{tabular}{lc}
\toprule
\textbf{Encoding} & \textbf{Similarity} \\
\midrule
Role-Filler & 0.31 \\
Shift-Based & 0.71 \\
Hybrid (no cancel) & 0.71 \\
Hybrid + Cancel & \textbf{0.92} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Similarity vs. arity (1 bound argument)}
\label{tab:arity-scaling}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Arity} & \textbf{Role-Filler} & \textbf{Shift} & \textbf{Hybrid+Cancel} \\
\midrule
2 & 0.31 & 0.71 & 0.92 \\
3 & 0.22 & 0.58 & 0.70 \\
4 & 0.18 & 0.50 & 0.50 \\
\bottomrule
\end{tabular}
\end{table}

Cancellation effectiveness decreases with arity: for $k=4$ with 1 bound argument, only 1 of 3 interference terms is removed.

\textbf{Reasoning Tasks}: We validate VSAR on transitive closure ($\texttt{ancestor}$ from $\texttt{parent}$: 12/12 correct, similarity 0.89), organizational hierarchy (49/49 subordinates retrieved, 0.84), and negation-as-failure ($\texttt{friendly} \leftarrow \texttt{not } \texttt{enemy}$: 16/16 correct, 0.92).

\section{Related Work}

Table~\ref{tab:comparison} compares VSAR to related approaches. Classical VSA work~\cite{plate2003holographic,kanerva2009hyperdimensional} lacks formal analysis of relational encoding. Neural-symbolic systems~\cite{garcez2012neural,evans2018learning} offer differentiability but poor interpretability. Logic programming~\cite{lloyd1987foundations,gelfond1988stable} provides exact reasoning but scales poorly. VSAR uniquely combines VSA operations with provable SNR bounds and explicit interference cancellation.

\begin{table}[t]
\centering
\caption{Comparison of reasoning approaches}
\label{tab:comparison}
\small
\begin{tabular}{lccccc}
\toprule
\textbf{System} & \textbf{Approx.} & \textbf{Vector.} & \textbf{Scalable} & \textbf{Formal} & \textbf{Cancel.} \\
\midrule
VSAR (ours) & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
Classical VSA & \checkmark & \checkmark & \checkmark & -- & -- \\
Prolog/ASP & -- & -- & -- & \checkmark & -- \\
Neural-symbolic & \checkmark & \checkmark & \checkmark & -- & -- \\
\bottomrule
\end{tabular}
\end{table}

\section{Conclusion}

We presented VSAR, a vector symbolic architecture for logical reasoning achieving both efficiency and accuracy. Our hybrid encoding combines predicate binding with shift-based positioning, enabling successive interference cancellation that exploits query constraints. We proved role-filler binding has fundamental SNR limitations ($1/\sqrt{k}$) and showed cancellation improves binary predicate retrieval from 31\% to 93\% similarity.

\textbf{Limitations}: VSAR currently supports single-variable queries only; multi-variable queries require joint decoding. High-arity predicates ($k > 4$) experience residual interference even with cancellation. Unlike symbolic reasoners, VSAR provides approximate answers with similarity-based confidence scores.

\textbf{Future directions}: Iterative refinement using decoded entities as constraints, attention mechanisms for fact weighting, learned encodings optimized for specific domains, and probabilistic interpretations of similarity scores.

VSAR demonstrates that VSA can serve as a practical substrate for scalable approximate reasoning, bridging symbolic and neural approaches.

\bibliographystyle{plain}
\bibliography{references}

\appendix

\section{VSA Background}
\label{app:vsa-background}

\begin{definition}[VSA Operations]
A VSA operates over $d$-dimensional vectors in $\C^d$ (for FHRR) or $\R^d$ (for other models). Hypervectors $\hvec{v} \in \C^d$ are normalized: $\norm{\hvec{v}}_2 = 1$.

Core operations:
\begin{itemize}
    \item \textbf{Bind} $\bind: \C^d \times \C^d \to \C^d$: Combines two vectors in a way that is approximately invertible. For FHRR, $\hvec{a} \bind \hvec{b} = \text{IFFT}(\text{FFT}(\hvec{a}) \odot \text{FFT}(\hvec{b}))$ (circular convolution).

    \item \textbf{Bundle} $\bundle: (\C^d)^k \to \C^d$: Superimposes multiple vectors. Typically $\bundle(\hvec{v}_1, \ldots, \hvec{v}_k) = \frac{1}{\sqrt{k}}\sum_{i=1}^k \hvec{v}_i$ (normalized sum).

    \item \textbf{Permute} $\shift_n: \C^d \to \C^d$: Circular shift by $n$ positions. $\shift_n(\hvec{v})$ rotates the vector elements cyclically.
\end{itemize}
\end{definition}

\textbf{Key properties}:
\begin{itemize}
    \item \emph{Approximate inverse}: $\hvec{a} \approx \text{unbind}(\hvec{a} \bind \hvec{b}, \hvec{b})$ with similarity $\sim 0.95$-$1.0$ for sufficient $d$.
    \item \emph{Exact inverse}: $\shift_{-n}(\shift_n(\hvec{v})) = \hvec{v}$ (permutation is perfectly invertible).
    \item \emph{Near-orthogonality}: Random hypervectors satisfy $\E[\hvec{a} \cdot \hvec{b}] \approx 0$ for $\hvec{a} \neq \hvec{b}$.
\end{itemize}

\section{Implementation Details}
\label{app:implementation}

\subsection{Architecture}

VSAR consists of three main components:

\begin{enumerate}
    \item \textbf{Symbol Registry}: Manages typed symbol spaces (entities, predicates, roles, etc.) with lazy registration of hypervectors.

    \item \textbf{Encoder}: Implements hybrid encoding using FHRR backend with dimension $d=8192$.

    \item \textbf{Retriever}: Performs queries with interference cancellation.
\end{enumerate}

\subsection{Encoding Details}

\textbf{Predicate binding}:
\begin{verbatim}
pred_vec = registry.get(PREDICATES, p)
args_bundle = sum(permute(entity_vec, i)
                  for i, entity_vec in args)
atom_vec = bind(pred_vec, args_bundle)
return normalize(atom_vec)
\end{verbatim}

\textbf{Critical}: Use \texttt{sum()} not \texttt{bundle()} to preserve linear superposition.

\textbf{Storage}: Each predicate maintains a separate list of fact vectors (no bundling across facts). This enables separate cleanup per fact, crucial for interference cancellation.

\subsection{Query Processing}

For query $p(t_1, \ldots, t_{i-1}, X, t_{i+1}, \ldots, t_k)$:

\begin{enumerate}
    \item Retrieve all facts for predicate $p$
    \item For each fact vector $\hvec{f}$:
    \begin{enumerate}
        \item Unbind predicate: $\hvec{a} = \text{unbind}(\hvec{f}, \hvec{p})$
        \item Verify bound arguments match (similarity $> 0.5$)
        \item Cancel known contributions: $\hvec{a}' = \hvec{a} - \sum_{j \neq i} \shift_j(\hvec{t}_j)$
        \item Decode: $\hvec{q}_i = \shift_{-i}(\hvec{a}')$
        \item Cleanup: $\hat{t}_i = \argmax_{t} \text{sim}(\hvec{q}_i, \hvec{t})$
    \end{enumerate}
    \item Aggregate results across all matching facts
\end{enumerate}

\subsection{Negation and Rules}

VSAR supports:
\begin{itemize}
    \item \textbf{Classical negation}: $\neg p(t_1, \ldots, t_k)$ stored as separate facts
    \item \textbf{Negation-as-failure}: $\texttt{not } p(X)$ in rule bodies
    \item \textbf{Horn rules}: $h \leftarrow b_1, \ldots, b_n$ with forward chaining
    \item \textbf{Stratified evaluation}: Ensures negation semantics
\end{itemize}

\section{Extended Proofs}

\subsection{Proof of Theorem \ref{thm:snr-rolebind} (Role-Filler SNR Bound)}
\label{app:proof-rolebind}

\begin{proof}[Extended Proof]
We analyze the decoded vector $\hvec{q}_j$ at position $j$ after role-filler unbinding.

\textbf{Step 1: Expand the decoded vector.}
From Eq.~\ref{eq:rolebind-decode}:
\begin{equation}
\hvec{q}_j = \hvec{t}_j + \sum_{i \neq j} \hvec{n}_i
\end{equation}
where $\hvec{n}_i = \text{unbind}(\hvec{\rho}_i \bind \hvec{t}_i, \hvec{\rho}_j) = \hvec{t}_i \bind \hvec{\rho}_i \bind \hvec{\rho}_j^\dagger$.

\textbf{Step 2: Noise term analysis.}
For $i \neq j$, the role vectors $\hvec{\rho}_i$ and $\hvec{\rho}_j$ are independent random hypervectors.
By the properties of random high-dimensional vectors:
\begin{itemize}
    \item $\E[\hvec{\rho}_i \cdot \hvec{\rho}_j] \approx 0$ for $i \neq j$
    \item $\E[\norm{\hvec{n}_i}^2] \approx 1$ (preserves norm in expectation)
    \item $\E[\hvec{n}_i \cdot \hvec{n}_{i'}] \approx 0$ for $i \neq i'$ (independent noise terms)
\end{itemize}

\textbf{Step 3: Signal and noise power.}
\begin{align}
\text{Signal power: } & \norm{\hvec{t}_j}^2 = 1 \\
\text{Noise power: } & \E\left[\norm{\sum_{i \neq j} \hvec{n}_i}^2\right] = \sum_{i \neq j} \E[\norm{\hvec{n}_i}^2] = k-1
\end{align}

The second equality follows from independence: cross-terms $\E[\hvec{n}_i \cdot \hvec{n}_{i'}]$ vanish.

\textbf{Step 4: Signal-to-noise ratio.}
\[
\text{SNR}_{\text{RF}} = \frac{\text{Signal power}}{\text{Noise power}} = \frac{1}{k-1}
\]

\textbf{Step 5: Expected cosine similarity.}
The magnitude of $\hvec{q}_j$ is:
\begin{equation}
\E[\norm{\hvec{q}_j}^2] = \E[\norm{\hvec{t}_j + \sum_{i \neq j} \hvec{n}_i}^2] = 1 + (k-1) = k
\end{equation}

The dot product with the target is:
\begin{align}
\E[\hvec{q}_j \cdot \hvec{t}_j] &= \E\left[\left(\hvec{t}_j + \sum_{i \neq j} \hvec{n}_i\right) \cdot \hvec{t}_j\right] \\
&= \norm{\hvec{t}_j}^2 + \sum_{i \neq j} \E[\hvec{n}_i \cdot \hvec{t}_j] \\
&= 1 + 0 = 1
\end{align}

Therefore:
\begin{equation}
\E[\text{sim}(\hvec{q}_j, \hvec{t}_j)] = \E\left[\frac{\hvec{q}_j \cdot \hvec{t}_j}{\norm{\hvec{q}_j} \norm{\hvec{t}_j}}\right] \approx \frac{1}{\sqrt{k}}
\end{equation}

\textbf{Numerical examples:}
\begin{itemize}
    \item Binary ($k=2$): $\text{SNR} = 1$, $\E[\text{sim}] \approx 0.707$
    \item Ternary ($k=3$): $\text{SNR} = 0.5$, $\E[\text{sim}] \approx 0.577$
    \item Quaternary ($k=4$): $\text{SNR} = 0.33$, $\E[\text{sim}] \approx 0.5$
\end{itemize}

This shows linear degradation with arity, confirming the fundamental limitation of role-filler binding.
\end{proof}

\subsection{Proof of Theorem \ref{thm:snr-hybrid} (Hybrid Encoding)}
\label{app:proof-hybrid}

\begin{proof}[Extended Proof]
We prove the three claims of Theorem~\ref{thm:snr-hybrid}.

\textbf{Claim 1: Predicate distinguishability.}

For predicates $p \neq p'$, the encoding yields:
\begin{align}
\hvec{f}_p &= \hvec{p} \bind \hvec{a} \\
\hvec{f}_{p'} &= \hvec{p}' \bind \hvec{a}
\end{align}
where $\hvec{a} = \sum_{i=1}^k \shift_i(\hvec{t}_i)$ is the same argument bundle.

The similarity is:
\begin{align}
\text{sim}(\hvec{f}_p, \hvec{f}_{p'}) &= \frac{(\hvec{p} \bind \hvec{a}) \cdot (\hvec{p}' \bind \hvec{a})}{\norm{\hvec{p} \bind \hvec{a}} \norm{\hvec{p}' \bind \hvec{a}}} \\
&\approx (\hvec{p} \cdot \hvec{p}') \cdot \E[\hvec{a} \cdot \hvec{a}] \\
&\approx 0
\end{align}

since $\hvec{p}$ and $\hvec{p}'$ are random orthogonal vectors with $\E[\hvec{p} \cdot \hvec{p}'] \approx 0$ for high $d$.

\textbf{Claim 2: Positional SNR same as shift-based.}

After unbinding the predicate:
\[
\hvec{a} = \text{unbind}(\hvec{f}, \hvec{p}) \approx \sum_{i=1}^k \shift_i(\hvec{t}_i) + \hvec{\epsilon}_{\text{unbind}}
\]

where $\norm{\hvec{\epsilon}_{\text{unbind}}} = \mathcal{O}(\epsilon)$ with $\epsilon \approx 0.05$ for FHRR at $d=8192$.

Decoding position $j$:
\begin{align}
\hvec{q}_j &= \shift_{-j}(\hvec{a}) \\
&\approx \shift_{-j}\left(\sum_{i=1}^k \shift_i(\hvec{t}_i)\right) + \shift_{-j}(\hvec{\epsilon}_{\text{unbind}}) \\
&= \hvec{t}_j + \sum_{i \neq j} \shift_{i-j}(\hvec{t}_i) + \shift_{-j}(\hvec{\epsilon}_{\text{unbind}})
\end{align}

The SNR is dominated by the positional interference terms $\shift_{i-j}(\hvec{t}_i)$, which have the same distribution as in pure shift-based encoding (Theorem~\ref{thm:snr-shift}).

\textbf{Claim 3: Negligible additional interference.}

The bind/unbind error contributes noise $\shift_{-j}(\hvec{\epsilon}_{\text{unbind}})$ with magnitude $\mathcal{O}(\epsilon)$.

Compared to the positional interference with power $(k-1)$, this is negligible:
\[
\frac{\epsilon^2}{k-1} \approx \frac{0.0025}{k-1} \ll 1
\]

For $k \geq 2$, the bind/unbind error is less than $0.25\%$ of the interference power.
\end{proof}

\subsection{Proof of Theorem \ref{thm:cancellation} (Interference Cancellation)}

\begin{proof}[Extended Proof]
We analyze successive interference cancellation step-by-step.

\textbf{Setup:} Query $p(t_1, \ldots, t_{i-1}, X, t_{i+1}, \ldots, t_k)$ with $m = k-1$ bound arguments.

\textbf{Step 1: After unbinding predicate.}
\begin{equation}
\hvec{a} = \text{unbind}(\hvec{f}, \hvec{p}) = \sum_{j=1}^k \shift_j(\hvec{t}_j) + \hvec{\epsilon}_{\text{unbind}}
\end{equation}

\textbf{Step 2: Cancel $m$ known arguments.}
For each bound position $j \neq i$, subtract $\shift_j(\hvec{t}_j)$:
\begin{equation}
\hvec{a}' = \hvec{a} - \sum_{j \neq i} \shift_j(\hvec{t}_j) = \shift_i(\hvec{t}_i) + \hvec{\epsilon}_{\text{unbind}}
\end{equation}

\textbf{Step 3: Decode the cleaned bundle.}
\begin{equation}
\hvec{q}_i = \shift_{-i}(\hvec{a}') = \hvec{t}_i + \shift_{-i}(\hvec{\epsilon}_{\text{unbind}})
\end{equation}

\textbf{Step 4: Noise power analysis.}
Without cancellation, the noise power was $(k-1)$ from positional interference.

With cancellation of $m = k-1$ terms, only the unbind error remains:
\begin{equation}
\text{Noise power} = \norm{\shift_{-i}(\hvec{\epsilon}_{\text{unbind}})}^2 = \norm{\hvec{\epsilon}_{\text{unbind}}}^2 \approx \epsilon^2
\end{equation}

\textbf{Step 5: Expected similarity.}
\begin{align}
\E[\norm{\hvec{q}_i}^2] &= \E[\norm{\hvec{t}_i + \shift_{-i}(\hvec{\epsilon}_{\text{unbind}})}^2] \\
&\approx 1 + \epsilon^2
\end{align}

\begin{align}
\E[\hvec{q}_i \cdot \hvec{t}_i] &= \E[(\hvec{t}_i + \shift_{-i}(\hvec{\epsilon}_{\text{unbind}})) \cdot \hvec{t}_i] \\
&= 1 + \E[\shift_{-i}(\hvec{\epsilon}_{\text{unbind}}) \cdot \hvec{t}_i] \\
&\approx 1
\end{align}

Therefore:
\begin{equation}
\E[\text{sim}(\hvec{q}_i, \hvec{t}_i)] = \frac{1}{\sqrt{1 + \epsilon^2}} \approx 1 - \frac{\epsilon^2}{2}
\end{equation}

For $\epsilon = 0.05$:
\[
\E[\text{sim}] \approx 1 - \frac{0.0025}{2} = 0.99875 \approx 0.999
\]

\textbf{General case:} For $m$ bound arguments out of $k$ total:
\begin{itemize}
    \item Remaining interference terms: $k - 1 - m$
    \item Noise power: $(k - 1 - m) + \epsilon^2$
    \item SNR: $\frac{1}{(k - 1 - m) + \epsilon^2}$
\end{itemize}

\textbf{Special cases:}
\begin{itemize}
    \item Binary, 1 bound ($k=2, m=1$): Noise power $\approx \epsilon^2$, similarity $\approx 0.999$
    \item Ternary, 2 bound ($k=3, m=2$): Noise power $\approx \epsilon^2$, similarity $\approx 0.999$
    \item Ternary, 1 bound ($k=3, m=1$): Noise power $\approx 1 + \epsilon^2$, similarity $\approx 0.707$
\end{itemize}

This shows that cancellation effectiveness scales with the number of bound arguments relative to arity.
\end{proof}

\subsection{Implementation Note: Linear Superposition}

\textbf{Critical requirement:} Interference cancellation requires exact linear superposition.

If the argument bundle uses normalized bundling:
\[
\hvec{a}_{\text{norm}} = \frac{1}{\sqrt{k}} \sum_{i=1}^k \shift_i(\hvec{t}_i)
\]

Then subtraction fails because magnitudes don't match:
\[
\hvec{a}_{\text{norm}} - \shift_j(\hvec{t}_j) \neq \frac{1}{\sqrt{k}} \left(\sum_{i \neq j} \shift_i(\hvec{t}_i)\right)
\]

This is why we use plain \texttt{sum()} not \texttt{bundle()} in the implementation.

\end{document}
